{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, optimizers\n",
        "\n",
        "class Estimator:\n",
        "    \"\"\"\n",
        "    Q-Value Estimator neural network.\n",
        "    This network is used for both the Q-Network and the Target Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scope=\"estimator\", summaries_dir=None, num_actions=4):\n",
        "        self.scope = scope\n",
        "        self.num_actions = num_actions\n",
        "        self.model = self._build_model()  # Build the Keras model\n",
        "        self.optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "        self.summary_writer = None\n",
        "\n",
        "        if summaries_dir:\n",
        "            summary_dir = os.path.join(summaries_dir, f\"summaries_{scope}\")\n",
        "            os.makedirs(summary_dir, exist_ok=True)\n",
        "            self.summary_writer = tf.summary.create_file_writer(summary_dir)\n",
        "\n",
        "    def _build_model(self):\n",
        "\n",
        "        model = models.Sequential([\n",
        "            layers.Input(shape=(84, 84, 4), name=\"X\"),\n",
        "            layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu'),\n",
        "            layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
        "            layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.Dense(self.num_actions, name=\"predictions\")\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def predict(self, s):\n",
        "        \"\"\"\n",
        "        Predicts action values.\n",
        "\n",
        "        Args:\n",
        "          s: State input of shape [batch_size, 84, 84, 4]\n",
        "\n",
        "        Returns:\n",
        "          Tensor of shape [batch_size, num_actions] containing the estimated action values.\n",
        "        \"\"\"\n",
        "        s = tf.convert_to_tensor(s, dtype=tf.float32) / 255.0\n",
        "        return self.model(s)\n",
        "\n",
        "    def update(self, s, a, y):\n",
        "        \"\"\"\n",
        "        Updates the estimator towards the given targets.\n",
        "\n",
        "        Args:\n",
        "          s: State input of shape [batch_size, 84, 84, 4]\n",
        "          a: Chosen actions of shape [batch_size]\n",
        "          y: Targets of shape [batch_size]\n",
        "\n",
        "        Returns:\n",
        "          The calculated loss on the batch.\n",
        "        \"\"\"\n",
        "        s = tf.convert_to_tensor(s, dtype=tf.float32) / 255.0\n",
        "        a = tf.convert_to_tensor(a, dtype=tf.int32)\n",
        "        y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Predict Q-values for all actions\n",
        "            q_values = self.model(s)\n",
        "            # Select Q-value of chosen actions\n",
        "            indices = tf.stack([tf.range(tf.shape(a)[0]), a], axis=1)\n",
        "            predicted_q = tf.gather_nd(q_values, indices)\n",
        "            # Compute loss\n",
        "            loss = losses.MeanSquaredError()(y, predicted_q)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        # Log the loss for TensorBoard\n",
        "        if self.summary_writer:\n",
        "            with self.summary_writer.as_default():\n",
        "                tf.summary.scalar(\"loss\", loss, step=tf.compat.v1.train.get_or_create_global_step())\n",
        "\n",
        "        return loss.numpy()\n"
      ],
      "metadata": {
        "id": "LQYL_UO83wl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BzVho25CNjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n",
        "import gym\n",
        "\n",
        "class StateProcessor:\n",
        "    \"\"\"\n",
        "    Processes game frames into the correct input format for the model.\n",
        "    Example: Resizing and converting to grayscale.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.resizer = tf.keras.layers.Resizing(84, 84)\n",
        "\n",
        "    def process(self, observation):\n",
        "        \"\"\"\n",
        "        Processes an observation frame (e.g., resizing, grayscaling).\n",
        "        Args:\n",
        "          observation: A raw frame from the environment.\n",
        "        Returns:\n",
        "          A processed frame of shape (84, 84, 1).\n",
        "        \"\"\"\n",
        "        observation = tf.image.rgb_to_grayscale(observation)\n",
        "        observation = self.resizer(observation)\n",
        "        return observation.numpy()\n",
        "\n",
        "# Initialize  objects\n",
        "sp = StateProcessor()\n",
        "e = Estimator(num_actions=4)\n",
        "\n",
        "env = gym.envs.make(\"Breakout-v0\")\n",
        "observation = env.reset()\n",
        "\n",
        "observation_p = sp.process(observation)\n",
        "\n",
        "# Create a stack of 4 identical frames as input\n",
        "observation = np.stack([observation_p] * 4, axis=2)\n",
        "observations = np.array([observation] * 2)\n",
        "\n",
        "predicted_q_values = e.predict(observations)\n",
        "print(\"Predicted Q-values:\", predicted_q_values)\n",
        "\n",
        "# Test training step\n",
        "targets = np.array([10.0, 10.0])\n",
        "actions = np.array([1, 3])\n",
        "loss = e.update(observations, actions, targets)\n",
        "print(\"Loss after update:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2WmpIfak5FB4",
        "outputId": "8e4f680b-f6ab-4f7e-b848-fd84c8f35823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameNotFound",
          "evalue": "Environment Breakout doesn't exist. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bd7342f6c479>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Breakout-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;34mf\"Environment {name} doesn't exist{namespace_msg}. {suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;31mNameNotFound\u001b[0m: Environment Breakout doesn't exist. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_model_parameters(sess, estimator1, estimator2):\n",
        "    \"\"\"\n",
        "    Copies the model parameters of one estimator to another.\n",
        "\n",
        "    Args:\n",
        "      sess: Tensorflow session instance\n",
        "      estimator1: Estimator to copy the paramters from\n",
        "      estimator2: Estimator to copy the parameters to\n",
        "    \"\"\"\n",
        "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
        "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
        "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
        "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
        "\n",
        "    update_ops = []\n",
        "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
        "        op = e2_v.assign(e1_v)\n",
        "        update_ops.append(op)\n",
        "\n",
        "    sess.run(update_ops)"
      ],
      "metadata": {
        "id": "5Kf1czwS38zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_epsilon_greedy_policy(estimator, nA):\n",
        "    \"\"\"\n",
        "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
        "\n",
        "    Args:\n",
        "        estimator: An estimator that returns q values for a given state\n",
        "        nA: Number of actions in the environment.\n",
        "\n",
        "    Returns:\n",
        "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
        "        the probabilities for each action in the form of a numpy array of length nA.\n",
        "\n",
        "    \"\"\"\n",
        "    def policy_fn(sess, observation, epsilon):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
        "        best_action = np.argmax(q_values)\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn"
      ],
      "metadata": {
        "id": "0TegEjQU1xjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_q_learning(sess,\n",
        "                    env,\n",
        "                    q_estimator,\n",
        "                    target_estimator,\n",
        "                    state_processor,\n",
        "                    num_episodes,\n",
        "                    experiment_dir,\n",
        "                    replay_memory_size=500000,\n",
        "                    replay_memory_init_size=50000,\n",
        "                    update_target_estimator_every=10000,\n",
        "                    discount_factor=0.99,\n",
        "                    epsilon_start=1.0,\n",
        "                    epsilon_end=0.1,\n",
        "                    epsilon_decay_steps=500000,\n",
        "                    batch_size=32,\n",
        "                    record_video_every=50):\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
        "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
        "\n",
        "    Args:\n",
        "        sess: Tensorflow Session object\n",
        "        env: OpenAI environment\n",
        "        q_estimator: Estimator object used for the q values\n",
        "        target_estimator: Estimator object used for the targets\n",
        "        state_processor: A StateProcessor object\n",
        "        num_episodes: Number of episodes to run for\n",
        "        experiment_dir: Directory to save Tensorflow summaries in\n",
        "        replay_memory_size: Size of the replay memory\n",
        "        replay_memory_init_size: Number of random experiences to sampel when initializing\n",
        "          the reply memory.\n",
        "        update_target_estimator_every: Copy parameters from the Q estimator to the\n",
        "          target estimator every N steps\n",
        "        discount_factor: Gamma discount factor\n",
        "        epsilon_start: Chance to sample a random action when taking an action.\n",
        "          Epsilon is decayed over time and this is the start value\n",
        "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
        "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
        "        batch_size: Size of batches to sample from the replay memory\n",
        "        record_video_every: Record a video every N episodes\n",
        "\n",
        "    Returns:\n",
        "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
        "    \"\"\"\n",
        "\n",
        "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    # The replay memory\n",
        "    replay_memory = []\n",
        "\n",
        "    # Keeps track of useful statistics\n",
        "    stats = plotting.EpisodeStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes))\n",
        "\n",
        "    # Create directories for checkpoints and summaries\n",
        "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
        "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    if not os.path.exists(monitor_path):\n",
        "        os.makedirs(monitor_path)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    # Load a previous checkpoint if we find one\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "    if latest_checkpoint:\n",
        "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
        "        saver.restore(sess, latest_checkpoint)\n",
        "\n",
        "    # Get the current time step\n",
        "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
        "\n",
        "    # The epsilon decay schedule\n",
        "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
        "\n",
        "    # The policy we're following\n",
        "    policy = make_epsilon_greedy_policy(\n",
        "        q_estimator,\n",
        "        len(VALID_ACTIONS))\n",
        "\n",
        "    # Populate the replay memory with initial experience\n",
        "    print(\"Populating replay memory...\")\n",
        "    state = env.reset()\n",
        "    state = state_processor.process(sess, state)\n",
        "    state = np.stack([state] * 4, axis=2)\n",
        "    for i in range(replay_memory_init_size):\n",
        "        action_probs = policy(sess, state, epsilons[total_t])\n",
        "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
        "        next_state = state_processor.process(sess, next_state)\n",
        "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
        "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            state = state_processor.process(sess, state)\n",
        "            state = np.stack([state] * 4, axis=2)\n",
        "        else:\n",
        "            state = next_state\n",
        "\n",
        "    # Record videos\n",
        "    env.monitor.start(monitor_path,\n",
        "                      resume=True,\n",
        "                      video_callable=lambda count: count % record_video_every == 0)\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "        # Save the current checkpoint\n",
        "        saver.save(tf.get_default_session(), checkpoint_path)\n",
        "\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "        state = state_processor.process(sess, state)\n",
        "        state = np.stack([state] * 4, axis=2)\n",
        "        loss = None\n",
        "\n",
        "        # One step in the environment\n",
        "        for t in itertools.count():\n",
        "\n",
        "            # Epsilon for this time step\n",
        "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
        "\n",
        "            # Add epsilon to Tensorboard\n",
        "            episode_summary = tf.Summary()\n",
        "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
        "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
        "\n",
        "            # Maybe update the target estimator\n",
        "            if total_t % update_target_estimator_every == 0:\n",
        "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
        "                print(\"\\nCopied model parameters to target network.\")\n",
        "\n",
        "            # Print out which step we're on, useful for debugging.\n",
        "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
        "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # Take a step\n",
        "            action_probs = policy(sess, state, epsilon)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
        "            next_state = state_processor.process(sess, next_state)\n",
        "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
        "\n",
        "            # If our replay memory is full, pop the first element\n",
        "            if len(replay_memory) == replay_memory_size:\n",
        "                replay_memory.pop(0)\n",
        "\n",
        "            # Save transition to replay memory\n",
        "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "            # Update statistics\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "\n",
        "            # Sample a minibatch from the replay memory\n",
        "            samples = random.sample(replay_memory, batch_size)\n",
        "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
        "\n",
        "            # Calculate q values and targets\n",
        "            # This is where Double Q-Learning comes in!\n",
        "            q_values_next = q_estimator.predict(sess, next_states_batch)\n",
        "            best_actions = np.argmax(q_values_next, axis=1)\n",
        "            q_values_next_target = target_estimator.predict(sess, next_states_batch)\n",
        "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * \\\n",
        "                discount_factor * q_values_next_target[np.arange(batch_size), best_actions]\n",
        "\n",
        "            # Perform gradient descent update\n",
        "            states_batch = np.array(states_batch)\n",
        "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "            total_t += 1\n",
        "\n",
        "        # Add summaries to tensorboard\n",
        "        episode_summary = tf.Summary()\n",
        "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
        "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
        "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
        "        q_estimator.summary_writer.flush()\n",
        "\n",
        "        yield total_t, plotting.EpisodeStats(\n",
        "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
        "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
        "\n",
        "    env.monitor.close()\n",
        "    return stats"
      ],
      "metadata": {
        "id": "avBqmoBA0mOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpReplay():\n",
        "    def __init__(self, e_max=15000, e_min=100):\n",
        "        self._max = e_max\n",
        "        self._min = e_min\n",
        "        self.exp = {'state':[], 'action':[], 'reward':[], 'next_state':[], 'done':[]} # total experiences the Agent stored\n",
        "\n",
        "    def get_max(self):\n",
        "        return self._max\n",
        "\n",
        "    def get_min(self):\n",
        "        return self._min\n",
        "\n",
        "    def get_num(self):\n",
        "        return len(self.exp['state'])\n",
        "\n",
        "    def get_batch(self, batch_size=64):\n",
        "        idx = np.random.choice(self.get_num(), size=batch_size, replace=False)\n",
        "        state = np.array([self.exp['state'][i] for i in idx])\n",
        "        action = [self.exp['action'][i] for i in idx]\n",
        "        reward = [self.exp['reward'][i] for i in idx]\n",
        "        next_state = np.array([self.exp['next_state'][i] for i in idx])\n",
        "        done = [self.exp['done'][i] for i in idx]\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        if self.get_num()>self.get_max():\n",
        "            del self.exp['state'][0]\n",
        "            del self.exp['action'][0]\n",
        "            del self.exp['reward'][0]\n",
        "            del self.exp['next_state'][0]\n",
        "            del self.exp['done'][0]\n",
        "\n",
        "        self.exp['state'].append(state)\n",
        "        self.exp['action'].append(action)\n",
        "        self.exp['reward'].append(reward)\n",
        "        self.exp['next_state'].append(next_state)\n",
        "        self.exp['done'].append(done)"
      ],
      "metadata": {
        "id": "WAzilnGT4_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TNET():\n",
        "    \"\"\"\n",
        "    Target network is for calculating the maximum estimated Q-value in given action a.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_units, out_units, hidden_units=250):\n",
        "        self.in_units = in_units\n",
        "        self.out_units = out_units\n",
        "        self.hidden_units = hidden_units\n",
        "        self._model()\n",
        "\n",
        "    def _model(self):\n",
        "        with tf.variable_scope('tnet'):\n",
        "            # input layer\n",
        "            self.x = tf.placeholder(tf.float32, shape=(None, self.in_units))\n",
        "\n",
        "            # from input layer to hidden layer1\n",
        "            W1=tf.get_variable('W1', shape=(self.in_units, self.hidden_units), initializer=tf.random_normal_initializer())\n",
        "            # from hidden layer1 to hiiden layer2\n",
        "            W2=tf.get_variable('W2', shape=(self.hidden_units, self.hidden_units), initializer=tf.random_normal_initializer())\n",
        "            # from hidden layer2 to output layer\n",
        "            W3=tf.get_variable('W3', shape=(self.hidden_units, self.out_units), initializer=tf.random_normal_initializer())\n",
        "\n",
        "            # the bias of hidden layer1\n",
        "            b1=tf.get_variable('b1', shape=(self.hidden_units), initializer=tf.zeros_initializer())\n",
        "            # the bias of hidden layer2\n",
        "            b2=tf.get_variable('b2', shape=(self.hidden_units), initializer=tf.zeros_initializer())\n",
        "\n",
        "            # the ouput of hidden layer1\n",
        "            h1=tf.nn.tanh(tf.matmul(self.x, W1)+b1)\n",
        "            # the output of hidden layer2\n",
        "            h2=tf.nn.tanh(tf.matmul(h1, W2)+b2)\n",
        "\n",
        "            # the output of output layer, that is, Q-value\n",
        "            self.q=tf.matmul(h2, W3)\n",
        "            self.params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='tnet')"
      ],
      "metadata": {
        "id": "jU-OrOn95Am-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNET():\n",
        "  def batch_train(self, batch_size=64):\n",
        "        \"\"\"Implement Double DQN Algorithm, batch training\"\"\"\n",
        "        if self.exp.get_num() < self.exp.get_min():\n",
        "\n",
        "            return\n",
        "        state, action, reward, next_state, done = self.exp.get_batch(batch_size)\n",
        "        state = state.reshape(batch_size, self.in_units)\n",
        "        next_state = next_state.reshape(batch_size, self.in_units)\n",
        "\n",
        "        qnet_q_values = self.session.run(self.q, feed_dict={self.x:next_state})\n",
        "        qnet_actions = np.argmax(qnet_q_values, axis=1)\n",
        "\n",
        "        # calculate estimated Q-values with qnet_actions by using Target-network\n",
        "        tnet_q_values = self.session.run(self.tnet.q, feed_dict={self.tnet.x:next_state})\n",
        "        tnet_q = [np.take(tnet_q_values[i], qnet_actions[i]) for i in range(batch_size)]\n",
        "\n",
        "        qnet_update_q = [r+0.95*q if not d else r for r, q, d in zip(reward, tnet_q, done)]\n",
        "\n",
        "        indices=[[i,action[i]] for i in range(batch_size)]\n",
        "        feed_dict={self.x:state, self.target:qnet_update_q, self.selected_idx:indices}\n",
        "        self.session.run(self.train_opt, feed_dict)"
      ],
      "metadata": {
        "id": "fQ37KPMS5EcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, env):\n",
        "\n",
        "        self.max_episodes = 10000\n",
        "        self.max_actions = 10000\n",
        "        self.exploration_rate = 1.0\n",
        "        self.exploration_decay = 0.0001\n",
        "        self.env = env\n",
        "        self.states = env.observation_space.shape[0]\n",
        "        self.actions = env.action_space.n\n",
        "\n",
        "        self.exp = ExpReplay()\n",
        "\n",
        "        self.batch_size = 64\n",
        "\n",
        "        # Deep Q Network\n",
        "        self.qnet = QNET(self.states, self.actions, self.exp)\n",
        "        session = tf.InteractiveSession()\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        self.qnet.set_session(session)\n",
        "\n",
        "    def train(self):\n",
        "        max_episodes = self.max_episodes\n",
        "        max_actions = self.max_actions\n",
        "        exploration_rate = self.exploration_rate\n",
        "        exploration_decay = self.exploration_decay\n",
        "        batch_size = self.batch_size\n",
        "\n",
        "        record_rewards = []\n",
        "        for i in range(max_episodes):\n",
        "            total_rewards = 0\n",
        "            state = self.env.reset()\n",
        "            state = state.reshape(1, self.states)\n",
        "            for j in range(max_actions):\n",
        "                self.env.render()\n",
        "                action = self.qnet.get_action(state, exploration_rate)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = next_state.reshape(1, self.states)\n",
        "                total_rewards += reward\n",
        "\n",
        "                if done:\n",
        "                    self.exp.add(state, action, (reward-100), next_state, done)\n",
        "                    self.qnet.batch_train(batch_size)\n",
        "                    break\n",
        "\n",
        "                self.exp.add(state, action, reward, next_state, done)\n",
        "                self.qnet.batch_train(batch_size)\n",
        "                if (j%25)== 0 and j>0:\n",
        "                    self.qnet.update()\n",
        "                state = next_state\n",
        "\n",
        "            record_rewards.append(total_rewards)\n",
        "            exploration_rate = 0.01 + (exploration_rate-0.01)*np.exp(-exploration_decay*(i+1))\n",
        "            if i%100==0 and i>0:\n",
        "                average_rewards = np.mean(np.array(record_rewards))\n",
        "                record_rewards = []\n",
        "                print(\"episodes: %i to %i, average_reward: %.3f, exploration: %.3f\" %(i-100, i, average_rewards, exploration_rate))\n"
      ],
      "metadata": {
        "id": "2n3ousQN5SQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "\n",
        "\n",
        "class DuelingQNetwork(tf.keras.Model, SamplingMixin):\n",
        "\n",
        "    def __init__(self, actions_space):\n",
        "\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        self.action_space = actions_space\n",
        "        self.conv1 = kl.Conv2D(32, 8, strides=4, activation=\"relu\",\n",
        "                               kernel_initializer=\"he_normal\")\n",
        "        self.conv2 = kl.Conv2D(64, 4, strides=2, activation=\"relu\",\n",
        "                               kernel_initializer=\"he_normal\")\n",
        "        self.conv3 = kl.Conv2D(64, 3, strides=1, activation=\"relu\",\n",
        "                               kernel_initializer=\"he_normal\")\n",
        "        self.flatten1 = kl.Flatten()\n",
        "        self.dense1 = kl.Dense(512, activation=\"relu\",\n",
        "                               kernel_initializer=\"he_normal\")\n",
        "        self.value = kl.Dense(1, kernel_initializer=\"he_normal\")\n",
        "        self.dense2 = kl.Dense(512, activation=\"relu\",\n",
        "                               kernel_initializer=\"he_normal\")\n",
        "        self.advantages = kl.Dense(self.action_space,\n",
        "                                   kernel_initializer=\"he_normal\")\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten1(x)\n",
        "\n",
        "        x1 = self.dense1(x)\n",
        "        value = self.value(x1)\n",
        "\n",
        "        x2 = self.dense2(x)\n",
        "        advantages = self.advantages(x2)\n",
        "        advantages_scaled = advantages - tf.reduce_mean(advantages, axis=1, keepdims=True)\n",
        "\n",
        "        q = value + advantages_scaled\n",
        "\n",
        "        return q"
      ],
      "metadata": {
        "id": "xK75YR0G4AGH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "dd5a1522-b420-40d5-f76d-245dbbd50f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SamplingMixin' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0446cfbb529e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDuelingQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSamplingMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SamplingMixin' is not defined"
          ]
        }
      ]
    }
  ]
}
